{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20221110_084724\\\"\n",
      "Presets specified: ['high_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 43200s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20221110_084724\\\"\n",
      "AutoGluon Version:  0.5.2\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    4701217\n",
      "Train Data Columns: 47\n",
      "Label Column: target\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == float, but few unique label-values observed and label-values can be converted to int).\n",
      "\tFirst 10 (of 102) unique label values:  [52.0, 30.0, 61.0, 20.0, 38.0, 28.0, 39.0, 14.0, 47.0, 40.0]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Tabular_complete========================\n",
      "==================Tabular_complete========================\n",
      "==================Tabular_complete========================\n",
      "==================Tabular_complete========================\n",
      "==================Tabular_complete========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 92 out of 102 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9999936186736328\n",
      "Train Data Class Count: 92\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    110434.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1504.38 MB (1.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 12 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 26 | ['maximum_speed_limit', 'weight_restricted', 'start_latitude', 'start_longitude', 'end_latitude', ...]\n",
      "\t\t('int', [])   : 21 | ['day_of_week', 'base_hour', 'lane_count', 'road_rating', 'road_name', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 21 | ['maximum_speed_limit', 'weight_restricted', 'start_latitude', 'start_longitude', 'end_latitude', ...]\n",
      "\t\t('int', [])       : 14 | ['day_of_week', 'base_hour', 'lane_count', 'road_rating', 'road_name', ...]\n",
      "\t\t('int', ['bool']) : 12 | ['multi_linked', 'connect_code', 'road_type', 'start_turn_restricted', 'end_turn_restricted', ...]\n",
      "\t10.9s = Fit runtime\n",
      "\t47 features in original data used to generate 47 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1184.7 MB (1.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 14.01s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 28783.46s of the 43185.99s of remaining time.\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t-4.1109\t = Validation score   (-mean_absolute_error)\n",
      "\t5852.14s\t = Training   runtime\n",
      "\t28.94s\t = Validation runtime\n",
      "Fitting model: RandomForest_BAG_L1 ... Training model for up to 22890.86s of the 37293.39s of remaining time.\n",
      "\tWarning: Model is expected to require 254.24% of available memory (Estimated before training)...\n",
      "\tNot enough memory to train RandomForest_BAG_L1... Skipping this model.\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 22887.93s of the 37290.45s of remaining time.\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "c:\\mb_job\\project\\Competition\\DACON\\jeju_traffic_prediction\\jeju_project\\lib\\site-packages\\xgboost\\compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-4.2158\t = Validation score   (-mean_absolute_error)\n",
      "\t970.08s\t = Training   runtime\n",
      "\t35.67s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_BAG_L1 ... Training model for up to 21872.33s of the 36274.86s of remaining time.\n",
      "\tWarning: Model is expected to require 255.14% of available memory (Estimated before training)...\n",
      "\tNot enough memory to train ExtraTrees_BAG_L1... Skipping this model.\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 21869.4s of the 36271.93s of remaining time.\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:43:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:20:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:59:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:38:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:19:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:02:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:47:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:37:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-3.4115\t = Validation score   (-mean_absolute_error)\n",
      "\t20653.9s\t = Training   runtime\n",
      "\t464.65s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 2878.35s of the 15127.58s of remaining time.\n",
      "Warning: Ensemble Selection ran out of time, early stopping at iteration 97. This may mean that the time_limit specified is very small for this problem.\n",
      "\t-3.4089\t = Validation score   (-mean_absolute_error)\n",
      "\t2930.49s\t = Training   runtime\n",
      "\t1.78s\t = Validation runtime\n",
      "Fitting 5 L2 models ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 12189.06s of the 12173.76s of remaining time.\n",
      "\tMemory not enough to fit LGBModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Not enough memory to safely train model, roughly requires: 168.326 GB, but only 100.048 GB is available...\n",
      "\tNot enough memory to train LightGBM_BAG_L2... Skipping this model.\n",
      "Fitting model: RandomForest_BAG_L2 ... Training model for up to 12036.88s of the 12021.84s of remaining time.\n",
      "\tWarning: Model is expected to require 270.36% of available memory (Estimated before training)...\n",
      "\tNot enough memory to train RandomForest_BAG_L2... Skipping this model.\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 11935.07s of the 11920.17s of remaining time.\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Not enough memory to safely train CatBoost model, roughly requires: 168.326 GB, but only 100.013 GB is available...\n",
      "\tNot enough memory to train CatBoost_BAG_L2... Skipping this model.\n",
      "Fitting model: ExtraTrees_BAG_L2 ... Training model for up to 11784.23s of the 11769.55s of remaining time.\n",
      "\tWarning: Model is expected to require 270.5% of available memory (Estimated before training)...\n",
      "\tNot enough memory to train ExtraTrees_BAG_L2... Skipping this model.\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 11682.65s of the 11667.79s of remaining time.\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Not enough memory to safely train XGBoost model, roughly requires: 168.326 GB, but only 99.942 GB is available...\n",
      "\tNot enough memory to train XGBoost_BAG_L2... Skipping this model.\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "No base models to train on, skipping auxiliary stack level 3...\n",
      "AutoGluon training complete, total runtime = 31682.55s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\t318.93s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t124.49s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:44:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t2808.86s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t2930.49s\t = Training   runtime\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20221110_084724\\\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================learning_complete========================\n",
      "==================learning_complete========================\n",
      "==================learning_complete========================\n",
      "==================learning_complete========================\n",
      "==================learning_complete========================\n",
      "==================learning_complete========================\n",
      "==================predictor_complete========================\n",
      "==================predictor_complete========================\n",
      "==================predictor_complete========================\n",
      "==================predictor_complete========================\n",
      "==================predictor_complete========================\n",
      "==================submission_complete========================\n",
      "==================submission_complete========================\n",
      "==================submission_complete========================\n",
      "==================submission_complete========================\n",
      "==================submission_complete========================\n",
      "==================submission_complete========================\n"
     ]
    }
   ],
   "source": [
    "#AutoGluon 쓸때는 numpy 1.23으로\n",
    "#pycaret 쓸때는 numpy 1.20으로\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "\n",
    "\n",
    "train = pd.read_parquet('./jeju_data/train_48_T40.parquet')\n",
    "test = pd.read_parquet('./jeju_data/test_48_T40.parquet')\n",
    "train = train.drop([\"tour_count\"], axis=1)\n",
    "test = test.drop([\"tour_count\"], axis=1)\n",
    "\n",
    "\n",
    "#######################Modeling########################\n",
    "\n",
    "\n",
    "\n",
    "train_data = TabularDataset(train)\n",
    "test_data = TabularDataset(test)\n",
    "\n",
    "#train_data = TabularDataset('./train_data.csv')\n",
    "#test_data = TabularDataset('./test_data.csv')\n",
    "\n",
    "\n",
    "print(\"==================Tabular_complete========================\")\n",
    "print(\"==================Tabular_complete========================\")\n",
    "print(\"==================Tabular_complete========================\")\n",
    "print(\"==================Tabular_complete========================\")\n",
    "print(\"==================Tabular_complete========================\")\n",
    "\n",
    "#save_path = 'Models-predict'  # specifies folder to store trained models\n",
    "\n",
    "predictor = TabularPredictor(label='target',  eval_metric='mean_absolute_error').fit(train_data, time_limit=43200, presets='high_quality',  ag_args_fit={'num_gpus': 1},\n",
    "                                                                                     hyperparameters={'RF':{}, 'GBM':{},'XGB':{},'XT':{},'CAT':{}})\n",
    "\n",
    "\n",
    "print(\"==================learning_complete========================\")\n",
    "print(\"==================learning_complete========================\")\n",
    "print(\"==================learning_complete========================\")\n",
    "print(\"==================learning_complete========================\")\n",
    "print(\"==================learning_complete========================\")\n",
    "print(\"==================learning_complete========================\")\n",
    "\n",
    "\n",
    "#predictor = TabularPredictor.load(save_path)  # unnecessary, just demonstrates how to load previously-trained predictor from file\n",
    "\n",
    "y_pred = predictor.predict(test_data)\n",
    "\n",
    "print(\"==================predictor_complete========================\")\n",
    "print(\"==================predictor_complete========================\")\n",
    "print(\"==================predictor_complete========================\")\n",
    "print(\"==================predictor_complete========================\")\n",
    "print(\"==================predictor_complete========================\")\n",
    "\n",
    "y_pred_final = pd.DataFrame(y_pred, columns=['target'])\n",
    "\n",
    "#######################submission########################\n",
    "sample_submission = pd.read_csv('./jeju_data/sample_submission.csv')\n",
    "sample_submission['target'] = y_pred_final\n",
    "sample_submission.to_csv(\"./submit5_auto.csv\", index = False)\n",
    "\n",
    "\n",
    "print(\"==================submission_complete========================\")\n",
    "print(\"==================submission_complete========================\")\n",
    "print(\"==================submission_complete========================\")\n",
    "print(\"==================submission_complete========================\")\n",
    "print(\"==================submission_complete========================\")\n",
    "print(\"==================submission_complete========================\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                      model  score_val  pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       WeightedEnsemble_L2  -3.408886     502.089039  24554.463917                1.776408        2930.491662            2      False          4\n",
      "1            XGBoost_BAG_L1  -3.411539     464.647442  20653.895147              464.647442       20653.895147            1      False          3\n",
      "2           LightGBM_BAG_L1  -4.110949      28.943731   5852.138326               28.943731        5852.138326            1      False          1\n",
      "3           CatBoost_BAG_L1  -4.215798      35.665188    970.077108               35.665188         970.077108            1      False          2\n",
      "4       XGBoost_BAG_L1_FULL        NaN            NaN   2808.862667                     NaN        2808.862667            1       True          7\n",
      "5  WeightedEnsemble_L2_FULL        NaN            NaN   5863.844470                     NaN        2930.491662            2       True          8\n",
      "6      LightGBM_BAG_L1_FULL        NaN            NaN    318.931209                     NaN         318.931209            1       True          5\n",
      "7      CatBoost_BAG_L1_FULL        NaN            NaN    124.490141                     NaN         124.490141            1       True          6\n",
      "Number of models trained: 8\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_CatBoost', 'WeightedEnsembleModel', 'StackerEnsembleModel_XGBoost', 'StackerEnsembleModel_LGB'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 21 | ['maximum_speed_limit', 'weight_restricted', 'start_latitude', 'start_longitude', 'end_latitude', ...]\n",
      "('int', [])       : 14 | ['day_of_week', 'base_hour', 'lane_count', 'road_rating', 'road_name', ...]\n",
      "('int', ['bool']) : 12 | ['multi_linked', 'connect_code', 'road_type', 'start_turn_restricted', 'end_turn_restricted', ...]\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\mb_job\\project\\Competition\\DACON\\jeju_traffic_prediction\\jeju_project\\lib\\site-packages\\autogluon\\core\\utils\\plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'LightGBM_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'CatBoost_BAG_L1': 'StackerEnsembleModel_CatBoost',\n",
       "  'XGBoost_BAG_L1': 'StackerEnsembleModel_XGBoost',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel',\n",
       "  'LightGBM_BAG_L1_FULL': 'StackerEnsembleModel_LGB',\n",
       "  'CatBoost_BAG_L1_FULL': 'StackerEnsembleModel_CatBoost',\n",
       "  'XGBoost_BAG_L1_FULL': 'StackerEnsembleModel_XGBoost',\n",
       "  'WeightedEnsemble_L2_FULL': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'LightGBM_BAG_L1': -4.110948575327891,\n",
       "  'CatBoost_BAG_L1': -4.215797627280089,\n",
       "  'XGBoost_BAG_L1': -3.411538830512379,\n",
       "  'WeightedEnsemble_L2': -3.408885883501337,\n",
       "  'LightGBM_BAG_L1_FULL': None,\n",
       "  'CatBoost_BAG_L1_FULL': None,\n",
       "  'XGBoost_BAG_L1_FULL': None,\n",
       "  'WeightedEnsemble_L2_FULL': None},\n",
       " 'model_best': 'WeightedEnsemble_L2_FULL',\n",
       " 'model_paths': {'LightGBM_BAG_L1': 'AutogluonModels/ag-20221110_084724\\\\models\\\\LightGBM_BAG_L1\\\\',\n",
       "  'CatBoost_BAG_L1': 'AutogluonModels/ag-20221110_084724\\\\models\\\\CatBoost_BAG_L1\\\\',\n",
       "  'XGBoost_BAG_L1': 'AutogluonModels/ag-20221110_084724\\\\models\\\\XGBoost_BAG_L1\\\\',\n",
       "  'WeightedEnsemble_L2': 'AutogluonModels/ag-20221110_084724\\\\models\\\\WeightedEnsemble_L2\\\\',\n",
       "  'LightGBM_BAG_L1_FULL': 'AutogluonModels/ag-20221110_084724\\\\models\\\\LightGBM_BAG_L1_FULL\\\\',\n",
       "  'CatBoost_BAG_L1_FULL': 'AutogluonModels/ag-20221110_084724\\\\models\\\\CatBoost_BAG_L1_FULL\\\\',\n",
       "  'XGBoost_BAG_L1_FULL': 'AutogluonModels/ag-20221110_084724\\\\models\\\\XGBoost_BAG_L1_FULL\\\\',\n",
       "  'WeightedEnsemble_L2_FULL': 'AutogluonModels/ag-20221110_084724\\\\models\\\\WeightedEnsemble_L2_FULL\\\\'},\n",
       " 'model_fit_times': {'LightGBM_BAG_L1': 5852.138326406479,\n",
       "  'CatBoost_BAG_L1': 970.0771081447601,\n",
       "  'XGBoost_BAG_L1': 20653.89514684677,\n",
       "  'WeightedEnsemble_L2': 2930.4916620254517,\n",
       "  'LightGBM_BAG_L1_FULL': 318.93120861053467,\n",
       "  'CatBoost_BAG_L1_FULL': 124.49014091491699,\n",
       "  'XGBoost_BAG_L1_FULL': 2808.8626670837402,\n",
       "  'WeightedEnsemble_L2_FULL': 2930.4916620254517},\n",
       " 'model_pred_times': {'LightGBM_BAG_L1': 28.94373106956482,\n",
       "  'CatBoost_BAG_L1': 35.66518783569336,\n",
       "  'XGBoost_BAG_L1': 464.64744234085083,\n",
       "  'WeightedEnsemble_L2': 1.7764084339141846,\n",
       "  'LightGBM_BAG_L1_FULL': None,\n",
       "  'CatBoost_BAG_L1_FULL': None,\n",
       "  'XGBoost_BAG_L1_FULL': None,\n",
       "  'WeightedEnsemble_L2_FULL': None},\n",
       " 'num_bag_folds': 8,\n",
       " 'max_stack_level': 2,\n",
       " 'num_classes': 92,\n",
       " 'model_hyperparams': {'LightGBM_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': False},\n",
       "  'CatBoost_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': False},\n",
       "  'XGBoost_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': False},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBM_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'CatBoost_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'XGBoost_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L2_FULL': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                       model  score_val  pred_time_val      fit_time  \\\n",
       " 0       WeightedEnsemble_L2  -3.408886     502.089039  24554.463917   \n",
       " 1            XGBoost_BAG_L1  -3.411539     464.647442  20653.895147   \n",
       " 2           LightGBM_BAG_L1  -4.110949      28.943731   5852.138326   \n",
       " 3           CatBoost_BAG_L1  -4.215798      35.665188    970.077108   \n",
       " 4       XGBoost_BAG_L1_FULL        NaN            NaN   2808.862667   \n",
       " 5  WeightedEnsemble_L2_FULL        NaN            NaN   5863.844470   \n",
       " 6      LightGBM_BAG_L1_FULL        NaN            NaN    318.931209   \n",
       " 7      CatBoost_BAG_L1_FULL        NaN            NaN    124.490141   \n",
       " \n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                1.776408        2930.491662            2      False   \n",
       " 1              464.647442       20653.895147            1      False   \n",
       " 2               28.943731        5852.138326            1      False   \n",
       " 3               35.665188         970.077108            1      False   \n",
       " 4                     NaN        2808.862667            1       True   \n",
       " 5                     NaN        2930.491662            2       True   \n",
       " 6                     NaN         318.931209            1       True   \n",
       " 7                     NaN         124.490141            1       True   \n",
       " \n",
       "    fit_order  \n",
       " 0          4  \n",
       " 1          3  \n",
       " 2          1  \n",
       " 3          2  \n",
       " 4          7  \n",
       " 5          8  \n",
       " 6          5  \n",
       " 7          6  }"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('jeju_project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef7138665f101e40c36832e601592e02514941853cb5231009ef9ffda5a7b00a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
